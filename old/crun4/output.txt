running job
		 loss:  0.366390198469162
		 loss:  2.9412503242492676
		 loss:  0.3442511260509491
		 loss:  0.03391418606042862
		 loss:  0.021776843816041946
		 loss:  4.147472858428955
		 loss:  1.0069692134857178
		 loss:  0.05377165228128433
		 loss:  0.029522085562348366
		 loss:  0.020142648369073868
		 loss:  0.015172193758189678
		 loss:  0.012092376127839088
		 loss:  0.010011222213506699
		 loss:  0.008516539819538593
		 loss:  0.007396338041871786
		 loss:  0.00652006920427084
		 loss:  0.005819994490593672
		 loss:  0.005248576868325472
		 loss:  5.346962928771973
		 loss:  0.5718263983726501
		 loss:  0.07644326984882355
		 loss:  0.0349162220954895
		 loss:  0.022529713809490204
		 loss:  0.01647523045539856
		 loss:  0.012890581972897053
		 loss:  0.010563530027866364
		 loss:  0.008897184394299984
		 loss:  4.872541904449463
		 loss:  0.7174854278564453
		 loss:  1.4225032329559326
		 loss:  1.1173357963562012
		 loss:  0.5166844129562378
		 loss:  0.08176777511835098
		 loss:  0.03705468401312828
		 loss:  0.023710986599326134
		 loss:  0.017279906198382378
		 loss:  0.013491523452103138
		 loss:  0.011010957881808281
		 loss:  0.009274731390178204
		 loss:  0.00798214040696621
		 loss:  0.006995589472353458
		 loss:  0.006211026106029749
		 loss:  0.005580086261034012
		 loss:  0.0050581106916069984
		 loss:  0.004618216305971146
		 loss:  0.004246504046022892
		 loss:  0.003927139099687338
		 loss:  0.0036499565467238426
		 loss:  0.0034083391074091196
		 loss:  0.003193043638020754
		 loss:  0.003002661047503352
		 loss:  0.002833995968103409
		 loss:  0.002680401550605893
		 loss:  5.976302146911621
		 loss:  0.5229470133781433
		 loss:  2.6157774925231934
		 loss:  1.1008727550506592
		 loss:  0.4475262761116028
		 loss:  0.06786556541919708
		 loss:  0.033705130219459534
		 loss:  0.02219771407544613
		 loss:  0.016406161710619926
		 loss:  0.01293165236711502
		 loss:  0.010622034780681133
		 loss:  0.008983432315289974
		 loss:  0.007764042820781469
		 loss:  4.991199493408203
		 loss:  0.6725999712944031
		 loss:  0.1031336784362793
		 loss:  0.04008124768733978
		 loss:  0.024788301438093185
		 loss:  0.017782192677259445
		 loss:  0.013772223144769669
		 loss:  0.011184142902493477
		 loss:  0.009376417845487595
		 loss:  0.008056879043579102
		 loss:  0.007044360041618347
		 loss:  0.00624680332839489
		 loss:  0.005603202618658543
		 loss:  0.005073648411780596
		 loss:  0.004627234302461147
		 loss:  0.004255288280546665
		 loss:  5.5403361320495605
		 loss:  0.6472234725952148
		 loss:  0.08882176876068115

Epoch: 1 -- Train: 0.553957462310791, Loss: 0.3343863785266876 Accuracy: tensor([0.0455])
running job
		 loss:  0.03744196146726608
		 loss:  0.0247452724725008
		 loss:  0.01833748258650303
		 loss:  0.014467813074588776
		 loss:  0.011897084303200245
		 loss:  0.010063386522233486
		 loss:  0.008694885298609734
		 loss:  0.007636047434061766
		 loss:  4.994439601898193
		 loss:  1.0917044878005981
		 loss:  1.0119926929473877
		 loss:  1.377333164215088
		 loss:  0.3711692988872528
		 loss:  1.3036164045333862
		 loss:  1.0707042217254639
		 loss:  0.7634353637695312
		 loss:  1.180480718612671
		 loss:  0.9410118460655212
		 loss:  0.5146601796150208
		 loss:  0.14527644217014313
		 loss:  0.058727480471134186
		 loss:  0.03518464416265488
		 loss:  0.02468421496450901
		 loss:  0.01882777363061905
		 loss:  0.015101508237421513
		 loss:  4.38479471206665
		 loss:  0.9016046524047852
		 loss:  0.4923247992992401
		 loss:  1.9992704391479492
		 loss:  1.0900577306747437
		 loss:  0.9726922512054443
		 loss:  0.7911903858184814
		 loss:  0.4927957355976105
		 loss:  0.1929563283920288
		 loss:  0.07868856191635132
		 loss:  0.04496653005480766
		 loss:  0.0305474903434515
		 loss:  3.795022964477539
		 loss:  0.7858944535255432
		 loss:  0.5211345553398132
		 loss:  0.22606609761714935
		 loss:  0.09079565107822418
		 loss:  0.05007992684841156
		 loss:  0.03332078456878662
		 loss:  0.02451370097696781
		 loss:  0.019188376143574715
		 loss:  0.015642210841178894
		 loss:  0.013135331682860851
		 loss:  0.011274673976004124
		 loss:  0.009848339483141899
		 loss:  0.008722774684429169
		 loss:  0.007810291834175587
		 loss:  4.956982612609863
		 loss:  0.6867207884788513
		 loss:  0.4046081006526947
		 loss:  0.1633225828409195
		 loss:  0.07416093349456787
		 loss:  0.04426492750644684
		 loss:  0.0305960513651371
		 loss:  3.782634973526001
		 loss:  0.7408481240272522
		 loss:  0.5135692358016968
		 loss:  0.2484169453382492
		 loss:  0.10448427498340607
		 loss:  0.05658309534192085
		 loss:  0.03704043850302696
		 loss:  0.026931216940283775
		 loss:  0.020890912041068077
		 loss:  0.016922131180763245
		 loss:  0.014123604632914066
		 loss:  0.012087782844901085
		 loss:  0.01052047498524189
		 loss:  0.009288077242672443
		 loss:  0.0082868542522192
		 loss:  0.007475498132407665
		 loss:  0.0067976415157318115
		 loss:  0.0062308101914823055
		 loss:  0.0057337102480232716
		 loss:  0.005311306565999985
		 loss:  0.004941631108522415
		 loss:  0.004613826051354408
		 loss:  0.004327694419771433
		 loss:  0.004076030571013689
		 loss:  0.0038459161296486855
		 loss:  0.0036354658659547567

Epoch: 2 -- Train: 0.5194852352142334, Loss: 0.5187241435050964 Accuracy: tensor([0.0455])
running job
		 loss:  0.0034512262791395187
		 loss:  0.0032974891364574432
		 loss:  0.0031558494083583355
		 loss:  5.802334785461426
		 loss:  0.7427589893341064
		 loss:  0.6171831488609314
		 loss:  1.0390321016311646
		 loss:  0.7475961446762085
		 loss:  0.6682530641555786
		 loss:  0.5566055774688721
		 loss:  0.40180641412734985
		 loss:  1.5521891117095947
		 loss:  0.683043897151947
		 loss:  0.6353667378425598
		 loss:  0.5812374949455261
		 loss:  0.914326548576355
		 loss:  0.6185716986656189
		 loss:  0.5748550295829773
		 loss:  0.5241552591323853
		 loss:  0.45922112464904785
		 loss:  0.3736935257911682
		 loss:  1.4331697225570679
		 loss:  0.5336059331893921
		 loss:  0.9319520592689514
		 loss:  0.5509500503540039
		 loss:  0.521144449710846
		 loss:  0.4923165440559387
		 loss:  0.4632174074649811
		 loss:  0.43187475204467773
		 loss:  0.3954554796218872
		 loss:  0.3506433963775635
		 loss:  0.2952979505062103
		 loss:  0.23236149549484253
		 loss:  0.17100846767425537
		 loss:  0.1212804988026619
		 loss:  0.08630698919296265
		 loss:  0.06333812326192856
		 loss:  0.04819582402706146
		 loss:  0.03798609972000122
		 loss:  0.030853265896439552
		 loss:  0.025666723027825356
		 loss:  3.837373971939087
		 loss:  0.38273704051971436
		 loss:  0.35729077458381653
		 loss:  0.3278028964996338
		 loss:  0.29231515526771545
		 loss:  0.2501925528049469
		 loss:  0.20352526009082794
		 loss:  0.15780025720596313
		 loss:  0.11859142780303955
		 loss:  0.08856987953186035
		 loss:  0.06720849871635437
		 loss:  0.052220746874809265
		 loss:  0.04160991311073303
		 loss:  0.03402065858244896
		 loss:  0.02838810160756111
		 loss:  0.024134650826454163
		 loss:  0.020816074684262276
		 loss:  0.018216921016573906
		 loss:  0.016102973371744156
		 loss:  0.014388381503522396
		 loss:  0.01295824721455574
		 loss:  0.011756186373531818
		 loss:  0.010729835368692875
		 loss:  0.009859198704361916
		 loss:  4.704654693603516
		 loss:  0.3167305290699005
		 loss:  0.29480838775634766
		 loss:  0.26927506923675537
		 loss:  0.23933692276477814
		 loss:  0.20535920560359955
		 loss:  0.16951142251491547
		 loss:  0.13517117500305176
		 loss:  2.3005001544952393
		 loss:  0.3078716993331909
		 loss:  0.2966141700744629
		 loss:  0.28601157665252686
		 loss:  0.2759915888309479
		 loss:  0.2664802074432373
		 loss:  0.25740155577659607
		 loss:  0.24865812063217163
		 loss:  0.24015238881111145
		 loss:  0.2317560911178589
		 loss:  0.22330890595912933
		 loss:  0.2146245241165161

Epoch: 3 -- Train: 0.5009599328041077, Loss: 0.33983370661735535 Accuracy: tensor([0.0455])
running job
		 loss:  0.20547132194042206
		 loss:  0.19661839306354523
		 loss:  0.18699419498443604
		 loss:  0.1764245480298996
		 loss:  0.1648084819316864
		 loss:  1.9576698541641235
		 loss:  0.2148594856262207
		 loss:  0.20913560688495636
		 loss:  0.20355190336704254
		 loss:  0.1980500966310501
		 loss:  0.1925697773694992
		 loss:  0.18706490099430084
		 loss:  0.1814405471086502
		 loss:  0.17561659216880798
		 loss:  0.16949865221977234
		 loss:  0.16297383606433868
		 loss:  0.15598788857460022
		 loss:  0.14842253923416138
		 loss:  0.14022569358348846
		 loss:  0.1314379721879959
		 loss:  0.12213554978370667
		 loss:  0.11242186278104782
		 loss:  2.3284544944763184
		 loss:  0.17444579303264618
		 loss:  0.1705811619758606
		 loss:  0.16682332754135132
		 loss:  0.16314387321472168
		 loss:  0.1595197319984436
		 loss:  1.9352607727050781
		 loss:  0.17692825198173523
		 loss:  0.17312105000019073
		 loss:  0.16945718228816986
		 loss:  0.16592749953269958
		 loss:  0.1625266671180725
		 loss:  1.9158865213394165
		 loss:  0.17869114875793457
		 loss:  0.1748250126838684
		 loss:  0.17110903561115265
		 loss:  0.16753628849983215
		 loss:  0.16409732401371002
		 loss:  1.9069944620132446
		 loss:  0.1802404224872589
		 loss:  1.8223509788513184
		 loss:  0.1971130222082138
		 loss:  0.19249622523784637
		 loss:  0.18807266652584076
		 loss:  1.7842448949813843
		 loss:  0.205259308218956
		 loss:  0.20029383897781372
		 loss:  0.19554205238819122
		 loss:  0.19099144637584686
		 loss:  0.18663015961647034
		 loss:  0.18244755268096924
		 loss:  0.17843319475650787
		 loss:  0.1745779663324356
		 loss:  0.17087322473526
		 loss:  0.16731052100658417
		 loss:  0.1638825237751007
		 loss:  1.9081668853759766
		 loss:  0.1800062656402588
		 loss:  0.17608903348445892
		 loss:  0.17232567071914673
		 loss:  0.16870763897895813
		 loss:  0.16522714495658875
		 loss:  0.16187705099582672
		 loss:  0.15865039825439453
		 loss:  0.15554095804691315
		 loss:  0.15254268050193787
		 loss:  0.14965005218982697
		 loss:  1.9908201694488525
		 loss:  0.16501520574092865
		 loss:  0.1616729348897934
		 loss:  0.15845374763011932
		 loss:  0.15535135567188263
		 loss:  0.15235988795757294
		 loss:  0.14947360754013062
		 loss:  0.1466875821352005
		 loss:  0.1439966857433319
		 loss:  0.14139655232429504
		 loss:  0.1388828009366989
		 loss:  0.1364513486623764
		 loss:  0.13409850001335144
		 loss:  0.1318204700946808
		 loss:  0.1296142041683197
		 loss:  0.12747620046138763

Epoch: 4 -- Train: 0.3566097021102905, Loss: 0.3083910346031189 Accuracy: tensor([0.0455])
running job
		 loss:  0.12540365755558014
		 loss:  0.12359338998794556
		 loss:  0.12183155119419098
		 loss:  2.1787490844726562
		 loss:  0.1339878886938095
		 loss:  0.13193915784358978
		 loss:  0.12994861602783203
		 loss:  0.12801367044448853
		 loss:  0.12613226473331451
		 loss:  0.12430203706026077
		 loss:  0.12252151221036911
		 loss:  0.12078820914030075
		 loss:  0.1191006749868393
		 loss:  0.11745724827051163
		 loss:  0.11585602909326553
		 loss:  0.11429573595523834
		 loss:  0.11277452856302261
		 loss:  0.11129143834114075
		 loss:  0.10984469950199127
		 loss:  0.108433298766613
		 loss:  0.10705610364675522
		 loss:  0.10571165382862091
		 loss:  0.10439879447221756
		 loss:  0.10311689972877502
		 loss:  0.10186436772346497
		 loss:  0.10064055770635605
		 loss:  0.09944459795951843
		 loss:  0.09827519953250885
		 loss:  0.09713190048933029
		 loss:  0.09601372480392456
		 loss:  0.09491977095603943
		 loss:  0.09384968876838684
		 loss:  0.09280192852020264
		 loss:  0.09177688509225845
		 loss:  0.09077289700508118
		 loss:  0.08978990465402603
		 loss:  0.08882656693458557
		 loss:  0.08788315206766129
		 loss:  2.485485792160034
		 loss:  0.0975508838891983
		 loss:  0.09642364829778671
		 loss:  0.0953209325671196
		 loss:  0.09424205869436264
		 loss:  0.09318645298480988
		 loss:  0.09215310961008072
		 loss:  0.09114144742488861
		 loss:  0.0901508629322052
		 loss:  0.08918078988790512
		 loss:  0.08823039382696152
		 loss:  0.08729930222034454
		 loss:  2.4918019771575928
		 loss:  0.09691672027111053
		 loss:  0.09580328315496445
		 loss:  0.09471408277750015
		 loss:  0.09364834427833557
		 loss:  0.09260515868663788
		 loss:  0.09158404916524887
		 loss:  0.09058443456888199
		 loss:  0.08960529416799545
		 loss:  0.08864635229110718
		 loss:  0.08770681172609329
		 loss:  0.0867861658334732
		 loss:  0.08588403463363647
		 loss:  0.08499959856271744
		 loss:  0.08413267135620117
		 loss:  2.5268714427948
		 loss:  2.416337013244629
		 loss:  0.10475033521652222
		 loss:  0.10346018522977829
		 loss:  0.10219979286193848
		 loss:  0.10096842050552368
		 loss:  0.09976497292518616
		 loss:  0.09858860075473785
		 loss:  2.3768579959869385
		 loss:  0.10910648852586746
		 loss:  2.2816579341888428
		 loss:  2.1765246391296387
		 loss:  0.13429874181747437
		 loss:  0.13224124908447266
		 loss:  0.13024209439754486
		 loss:  0.12829892337322235
		 loss:  0.12640953063964844
		 loss:  0.1245720311999321
		 loss:  0.1227840781211853
		 loss:  2.171513795852661

Epoch: 5 -- Train: 0.3416914641857147, Loss: 0.3108387887477875 Accuracy: tensor([0.0455])
running job
		 loss:  0.13500142097473145
		 loss:  0.1331302970647812
		 loss:  2.0951504707336426
		 loss:  0.14464522898197174
		 loss:  0.1425182819366455
		 loss:  2.0323166847229004
		 loss:  0.15451066195964813
		 loss:  0.15210743248462677
		 loss:  0.14977280795574188
		 loss:  0.14750410616397858
		 loss:  0.1452983021736145
		 loss:  0.14315322041511536
		 loss:  0.1410665661096573
		 loss:  0.13903583586215973
		 loss:  0.13705937564373016
		 loss:  0.1351347267627716
		 loss:  0.1332601010799408
		 loss:  0.13143348693847656
		 loss:  0.12965357303619385
		 loss:  2.1196415424346924
		 loss:  0.1409817785024643
		 loss:  0.13895346224308014
		 loss:  0.1369791328907013
		 loss:  0.13505640625953674
		 loss:  0.13318392634391785
		 loss:  0.13135947287082672
		 loss:  0.12958121299743652
		 loss:  0.1278478354215622
		 loss:  0.12615737318992615
		 loss:  0.1245085671544075
		 loss:  0.1228998452425003
		 loss:  0.12132991850376129
		 loss:  0.11979739367961884
		 loss:  0.11830085515975952
		 loss:  0.11683940887451172
		 loss:  0.11541161686182022
		 loss:  0.1140165701508522
		 loss:  0.11265300959348679
		 loss:  0.11131991446018219
		 loss:  0.11001653969287872
		 loss:  0.10874193161725998
		 loss:  2.2835779190063477
		 loss:  0.11883417516946793
		 loss:  0.11736015230417252
		 loss:  0.11592036485671997
		 loss:  2.2237708568573
		 loss:  0.126459538936615
		 loss:  0.12480323761701584
		 loss:  0.12318749725818634
		 loss:  0.12161063402891159
		 loss:  0.12007135152816772
		 loss:  2.190962791442871
		 loss:  0.13085798919200897
		 loss:  0.12909246981143951
		 loss:  0.127371147274971
		 loss:  0.12569260597229004
		 loss:  0.12405503541231155
		 loss:  0.12245717644691467
		 loss:  0.12089796364307404
		 loss:  2.184574842453003
		 loss:  0.13173280656337738
		 loss:  0.12994515895843506
		 loss:  0.1282026618719101
		 loss:  0.12650345265865326
		 loss:  0.12484616041183472
		 loss:  0.12322933971881866
		 loss:  0.12165138125419617
		 loss:  0.12011121213436127
		 loss:  0.11860740929841995
		 loss:  0.11713878065347672
		 loss:  0.11570408940315247
		 loss:  0.11430243402719498
		 loss:  0.11293234676122665
		 loss:  2.2481727600097656
		 loss:  0.12328846007585526
		 loss:  0.12170911580324173
		 loss:  0.1201675534248352
		 loss:  0.11866235733032227
		 loss:  2.2019615173339844
		 loss:  0.12936578691005707
		 loss:  0.1276378035545349
		 loss:  0.1259525567293167
		 loss:  0.12430877983570099
		 loss:  0.1227048933506012
		 loss:  0.1211395189166069

Epoch: 6 -- Train: 0.3439866304397583, Loss: 0.3071663975715637 Accuracy: tensor([0.0455])
running job
		 loss:  0.1196114644408226
		 loss:  0.11826781183481216
		 loss:  0.11695235967636108
		 loss:  0.11566437780857086
		 loss:  0.11440280824899673
		 loss:  0.11316710710525513
		 loss:  0.11195630580186844
		 loss:  0.11076997965574265
		 loss:  0.10960723459720612
		 loss:  0.10846741497516632
		 loss:  0.10734985768795013
		 loss:  0.10625387728214264
		 loss:  0.10517901182174683
		 loss:  0.10412469506263733
		 loss:  0.10309023410081863
		 loss:  0.1020752564072609
		 loss:  0.10107897222042084
		 loss:  0.10010121762752533
		 loss:  0.09914106875658035
		 loss:  0.09819858521223068
		 loss:  0.0972728431224823
		 loss:  0.09636378288269043
		 loss:  0.09547079354524612
		 loss:  0.09459339082241058
		 loss:  2.413822650909424
		 loss:  2.3260936737060547
		 loss:  0.11256588250398636
		 loss:  0.111367367208004
		 loss:  0.11019272357225418
		 loss:  0.10904128104448318
		 loss:  0.1079125925898552
		 loss:  0.10680586844682693
		 loss:  0.10572034120559692
		 loss:  0.10465575009584427
		 loss:  0.10361120104789734
		 loss:  0.102586530148983
		 loss:  0.10158073902130127
		 loss:  0.10059376806020737
		 loss:  0.09962482005357742
		 loss:  2.3648717403411865
		 loss:  0.10812734812498093
		 loss:  0.10701636970043182
		 loss:  0.10592688620090485
		 loss:  0.10485832393169403
		 loss:  0.10381001234054565
		 loss:  0.10278145968914032
		 loss:  0.10177220404148102
		 loss:  2.3447670936584473
		 loss:  0.11040528118610382
		 loss:  0.10924970358610153
		 loss:  0.10811685770750046
		 loss:  0.10700619220733643
		 loss:  0.10591690987348557
		 loss:  0.1048484519124031
		 loss:  2.316736936569214
		 loss:  0.11366518586874008
		 loss:  0.1124444454908371
		 loss:  0.11124823987483978
		 loss:  0.11007601767778397
		 loss:  0.10892700403928757
		 loss:  0.1078004315495491
		 loss:  2.290647029876709
		 loss:  0.1167895570397377
		 loss:  0.1155046597123146
		 loss:  0.11424660682678223
		 loss:  2.2362184524536133
		 loss:  0.12359929084777832
		 loss:  0.12217036634683609
		 loss:  0.12077224999666214
		 loss:  0.11940402537584305
		 loss:  0.1180647537112236
		 loss:  0.11675360053777695
		 loss:  0.11546961218118668
		 loss:  0.1142120435833931
		 loss:  0.11298026144504547
		 loss:  2.2466485500335693
		 loss:  0.12226299196481705
		 loss:  0.12086278945207596
		 loss:  0.11949267983436584
		 loss:  0.11815140396356583
		 loss:  0.11683845520019531
		 loss:  2.2152481079101562
		 loss:  0.126332625746727
		 loss:  0.12484385073184967
		 loss:  0.12338782846927643

Epoch: 7 -- Train: 0.3424307107925415, Loss: 0.30763912200927734 Accuracy: tensor([0.0455])
running job
		 loss:  0.12196345627307892
		 loss:  0.12070844322443008
		 loss:  0.11947754770517349
		 loss:  2.19333553314209
		 loss:  0.12811604142189026
		 loss:  2.12831974029541
		 loss:  0.13715209066867828
		 loss:  0.13558898866176605
		 loss:  0.13405920565128326
		 loss:  0.1325613111257553
		 loss:  0.1310945451259613
		 loss:  0.12965796887874603
		 loss:  0.1282505840063095
		 loss:  0.12687155604362488
		 loss:  0.1255202740430832
		 loss:  2.147351026535034
		 loss:  0.13443954288959503
		 loss:  0.13293367624282837
		 loss:  0.1314592957496643
		 loss:  0.13001519441604614
		 loss:  0.12860065698623657
		 loss:  0.12721465528011322
		 loss:  0.12585638463497162
		 loss:  0.12452530115842819
		 loss:  0.12322038412094116
		 loss:  2.16456937789917
		 loss:  0.1320343315601349
		 loss:  0.1305783987045288
		 loss:  0.1291522979736328
		 loss:  0.12775520980358124
		 loss:  0.12638621032238007
		 loss:  0.125044584274292
		 loss:  0.12372949719429016
		 loss:  0.12244009226560593
		 loss:  0.12117595970630646
		 loss:  0.11993612349033356
		 loss:  0.11872006207704544
		 loss:  0.11752699315547943
		 loss:  0.11635638028383255
		 loss:  2.2180705070495605
		 loss:  0.12484469264745712
		 loss:  0.12353344261646271
		 loss:  2.1622045040130615
		 loss:  0.1323619782924652
		 loss:  0.13089920580387115
		 loss:  0.12946653366088867
		 loss:  0.12806318700313568
		 loss:  0.12668798863887787
		 loss:  0.12534025311470032
		 loss:  0.1240193322300911
		 loss:  0.12272440642118454
		 loss:  0.12145461142063141
		 loss:  2.1780221462249756
		 loss:  0.13018642365932465
		 loss:  0.1287684589624405
		 loss:  0.1273791342973709
		 loss:  0.12601760029792786
		 loss:  2.143672466278076
		 loss:  0.13495934009552002
		 loss:  0.13344267010688782
		 loss:  0.1319577544927597
		 loss:  0.13050328195095062
		 loss:  0.12907874584197998
		 loss:  0.1276831328868866
		 loss:  0.12631550431251526
		 loss:  0.12497536092996597
		 loss:  0.12366154789924622
		 loss:  0.1223735362291336
		 loss:  0.1211106926202774
		 loss:  0.11987204849720001
		 loss:  0.11865727603435516
		 loss:  0.1174653023481369
		 loss:  0.11629589647054672
		 loss:  0.11514827609062195
		 loss:  0.11402188241481781
		 loss:  0.11291605234146118
		 loss:  0.11183053255081177
		 loss:  0.11076442897319794
		 loss:  0.1097174808382988
		 loss:  0.10868921875953674
		 loss:  0.10767894983291626
		 loss:  2.2907299995422363
		 loss:  0.11573095619678497
		 loss:  0.11459384858608246
		 loss:  0.11347758024930954

Epoch: 8 -- Train: 0.34213021397590637, Loss: 0.3059396743774414 Accuracy: tensor([0.0455])
running job
		 loss:  0.11238180845975876
		 loss:  0.1114129051566124
		 loss:  0.1104598268866539
		 loss:  0.10952208191156387
		 loss:  0.10859928280115128
		 loss:  0.10769104212522507
		 loss:  2.2897472381591797
		 loss:  2.2204883098602295
		 loss:  0.1235349178314209
		 loss:  0.1223774403333664
		 loss:  0.12124016135931015
		 loss:  0.12012273073196411
		 loss:  0.11902447789907455
		 loss:  0.11794503778219223
		 loss:  0.11688395589590073
		 loss:  0.11584074050188065
		 loss:  0.11481492966413498
		 loss:  0.11380626261234283
		 loss:  0.11281415075063705
		 loss:  2.2460989952087402
		 loss:  0.12027019262313843
		 loss:  0.11916935443878174
		 loss:  0.1180875301361084
		 loss:  0.11702404916286469
		 loss:  0.1159784346818924
		 loss:  0.11495032161474228
		 loss:  0.11393934488296509
		 loss:  0.11294512450695038
		 loss:  0.11196718364953995
		 loss:  0.11100513488054276
		 loss:  0.11005861312150955
		 loss:  2.26930832862854
		 loss:  0.11738961935043335
		 loss:  0.11633781343698502
		 loss:  0.11530379951000214
		 loss:  0.11428690701723099
		 loss:  0.11328696459531784
		 loss:  0.11230339109897614
		 loss:  0.11133591085672379
		 loss:  0.11038403958082199
		 loss:  0.10944750159978867
		 loss:  0.10852581262588501
		 loss:  0.10761889815330505
		 loss:  0.10672616213560104
		 loss:  0.10584742575883865
		 loss:  0.10498230159282684
		 loss:  0.10413049906492233
		 loss:  0.1032918393611908
		 loss:  0.10246581584215164
		 loss:  0.10165224969387054
		 loss:  0.10085095465183258
		 loss:  0.10006152093410492
		 loss:  0.09928388893604279
		 loss:  0.09851763397455215
		 loss:  2.3736953735351562
		 loss:  0.10529715567827225
		 loss:  0.10444068163633347
		 loss:  2.318596839904785
		 loss:  0.11150888353586197
		 loss:  0.11055418848991394
		 loss:  0.1096150353550911
		 loss:  0.10869071632623672
		 loss:  0.1077810600399971
		 loss:  2.288961410522461
		 loss:  0.1150069609284401
		 loss:  0.11399497836828232
		 loss:  0.11299984902143478
		 loss:  0.11202099919319153
		 loss:  0.11105794459581375
		 loss:  0.11011061072349548
		 loss:  0.10917841643095016
		 loss:  0.10826107859611511
		 loss:  0.10735831409692764
		 loss:  0.10646963119506836
		 loss:  0.10559485107660294
		 loss:  0.10473369807004929
		 loss:  0.10388565808534622
		 loss:  0.10305076837539673
		 loss:  0.10222842544317245
		 loss:  0.10141842812299728
		 loss:  0.10062072426080704
		 loss:  2.3537416458129883
		 loss:  0.10750403255224228
		 loss:  2.2913811206817627
		 loss:  0.11471683531999588

Epoch: 9 -- Train: 0.3417413532733917, Loss: 0.30613818764686584 Accuracy: tensor([0.0455])
running job
		 loss:  0.11370977014303207
		 loss:  0.11281798034906387
		 loss:  0.11193925887346268
		 loss:  0.11107362806797028
		 loss:  0.11022038012742996
		 loss:  0.10937944054603577
		 loss:  0.10855062305927277
		 loss:  0.10773375630378723
		 loss:  2.2885842323303223
		 loss:  0.11421417444944382
		 loss:  0.11331495642662048
		 loss:  0.11242889612913132
		 loss:  0.1115557998418808
		 loss:  0.11069570481777191
		 loss:  0.10984790325164795
		 loss:  0.10901252925395966
		 loss:  0.10818886756896973
		 loss:  0.1073770523071289
		 loss:  2.2917048931121826
		 loss:  0.11384265124797821
		 loss:  0.11294874548912048
		 loss:  0.11206821352243423
		 loss:  0.11120045930147171
		 loss:  0.11034539341926575
		 loss:  0.10950274765491486
		 loss:  0.10867221653461456
		 loss:  0.10785341262817383
		 loss:  0.1070464625954628
		 loss:  0.10625065863132477
		 loss:  0.10546612739562988
		 loss:  0.10469268262386322
		 loss:  0.10392970591783524
		 loss:  0.10317722707986832
		 loss:  2.32930588722229
		 loss:  0.10946385562419891
		 loss:  2.273597240447998
		 loss:  0.11601632833480835
		 loss:  0.11509004980325699
		 loss:  2.22654390335083
		 loss:  0.1218704879283905
		 loss:  0.12085434049367905
		 loss:  0.11985428631305695
		 loss:  0.11886952817440033
		 loss:  0.11790001392364502
		 loss:  0.11694525182247162
		 loss:  0.116005077958107
		 loss:  2.2191247940063477
		 loss:  0.12282150983810425
		 loss:  0.12179049104452133
		 loss:  0.1207757368683815
		 loss:  0.11977677047252655
		 loss:  0.1187933161854744
		 loss:  2.1968894004821777
		 loss:  0.12571845948696136
		 loss:  0.12464149296283722
		 loss:  0.12358170002698898
		 loss:  0.12253859639167786
		 loss:  0.12151214480400085
		 loss:  0.12050175666809082
		 loss:  0.11950695514678955
		 loss:  0.11852768063545227
		 loss:  0.11756345629692078
		 loss:  0.11661388725042343
		 loss:  0.11567860841751099
		 loss:  0.11475743353366852
		 loss:  0.11385009437799454
		 loss:  0.11295619606971741
		 loss:  2.244096517562866
		 loss:  0.11965123564004898
		 loss:  2.190160036087036
		 loss:  0.1266094297170639
		 loss:  0.12551796436309814
		 loss:  0.12444425374269485
		 loss:  0.12338751554489136
		 loss:  0.12234769761562347
		 loss:  0.12132410705089569
		 loss:  0.12031659483909607
		 loss:  0.11932478845119476
		 loss:  0.11834830045700073
		 loss:  0.11738675832748413
		 loss:  0.11643978208303452
		 loss:  0.11550720781087875
		 loss:  0.11458864063024521
		 loss:  0.1136838048696518
		 loss:  0.11279242485761642

Epoch: 10 -- Train: 0.34135901927948, Loss: 0.30587276816368103 Accuracy: tensor([0.0455])
running job
		 loss:  0.11191409826278687
		 loss:  2.252063274383545
		 loss:  0.11788465082645416
		 loss:  0.1170252114534378
		 loss:  0.11617767065763474
		 loss:  2.2169737815856934
		 loss:  0.12230128049850464
		 loss:  0.12138049304485321
		 loss:  0.12047259509563446
		 loss:  2.1829845905303955
		 loss:  0.12674565613269806
		 loss:  0.1257610321044922
		 loss:  2.1428637504577637
		 loss:  0.13221149146556854
		 loss:  0.13114598393440247
		 loss:  0.1300964206457138
		 loss:  0.1290625035762787
		 loss:  0.128043994307518
		 loss:  0.12704038619995117
		 loss:  0.12605144083499908
		 loss:  0.12507687509059906
		 loss:  0.12411642074584961
		 loss:  0.12316979467868805
		 loss:  0.12223661690950394
		 loss:  0.12131661176681519
		 loss:  0.12040960788726807
		 loss:  2.1834731101989746
		 loss:  0.12668053805828094
		 loss:  0.12569691240787506
		 loss:  0.12472747266292572
		 loss:  0.1237720474600792
		 loss:  0.12283026427030563
		 loss:  0.12190193682909012
		 loss:  0.1209866926074028
		 loss:  0.12008435279130936
		 loss:  0.11919453740119934
		 loss:  0.11831716448068619
		 loss:  0.11745184659957886
		 loss:  2.206752061843872
		 loss:  0.12362036108970642
		 loss:  0.12268074601888657
		 loss:  0.12175450474023819
		 loss:  0.12084134668111801
		 loss:  0.1199410930275917
		 loss:  0.11905326694250107
		 loss:  0.11817777901887894
		 loss:  0.11731445789337158
		 loss:  0.11646291613578796
		 loss:  0.11562296003103256
		 loss:  0.11479441821575165
		 loss:  0.11397689580917358
		 loss:  0.11317040771245956
		 loss:  0.11237466335296631
		 loss:  0.11158937960863113
		 loss:  0.11081447452306747
		 loss:  0.11004974693059921
		 loss:  0.10929490625858307
		 loss:  2.2743303775787354
		 loss:  0.11516888439655304
		 loss:  2.2251505851745605
		 loss:  0.12125684320926666
		 loss:  0.12035073339939117
		 loss:  0.11945713311433792
		 loss:  0.11857617646455765
		 loss:  0.1177072674036026
		 loss:  0.11685033142566681
		 loss:  0.116005077958107
		 loss:  0.11517132818698883
		 loss:  0.1143488958477974
		 loss:  0.11353739351034164
		 loss:  0.11273672431707382
		 loss:  0.11194682121276855
		 loss:  0.11116717755794525
		 loss:  0.11039770394563675
		 loss:  0.10963831841945648
		 loss:  0.10888884216547012
		 loss:  0.108148954808712
		 loss:  0.10741859674453735
		 loss:  0.10669755190610886
		 loss:  0.10598553717136383
		 loss:  0.10528255999088287
		 loss:  0.10458843410015106
		 loss:  0.10390274226665497
		 loss:  2.3220057487487793
		 loss:  0.1095724031329155

Epoch: 11 -- Train: 0.3407555818557739, Loss: 0.3054710626602173 Accuracy: tensor([0.0455])
running job
		 loss:  0.1088237315416336
		 loss:  0.10815837234258652
		 loss:  2.28352689743042
		 loss:  0.11339329183101654
		 loss:  0.11267420649528503
		 loss:  0.1119636669754982
		 loss:  0.11126156896352768
		 loss:  0.1105678454041481
		 loss:  0.10988207906484604
		 loss:  0.1092044934630394
		 loss:  0.10853468626737595
		 loss:  0.10787267237901688
		 loss:  0.10721835494041443
		 loss:  0.10657143592834473
		 loss:  0.10593192279338837
		 loss:  0.10529962182044983
		 loss:  0.10467443615198135
		 loss:  0.10405637323856354
		 loss:  0.1034451350569725
		 loss:  0.1028406172990799
		 loss:  0.10224273800849915
		 loss:  0.10165149718523026
		 loss:  0.10106690227985382
		 loss:  0.10048844665288925
		 loss:  0.09991644322872162
		 loss:  0.09935037791728973
		 loss:  0.09879046678543091
		 loss:  0.09823662042617798
		 loss:  0.09768863022327423
		 loss:  2.3797147274017334
		 loss:  0.10255727171897888
		 loss:  0.10196276754140854
		 loss:  0.10137448459863663
		 loss:  0.10079286247491837
		 loss:  0.10021747648715973
		 loss:  0.09964834153652191
		 loss:  0.09908514469861984
		 loss:  0.09852822124958038
		 loss:  0.09797714650630951
		 loss:  0.09743182361125946
		 loss:  0.09689226746559143
		 loss:  0.09635858237743378
		 loss:  2.392709970474243
		 loss:  0.10117874294519424
		 loss:  0.10059916228055954
		 loss:  0.10002582520246506
		 loss:  0.0994587391614914
		 loss:  0.09889770299196243
		 loss:  0.09834272414445877
		 loss:  0.09779349714517593
		 loss:  0.09725023806095123
		 loss:  0.09671274572610855
		 loss:  0.09618069976568222
		 loss:  0.09565433114767075
		 loss:  2.3996639251708984
		 loss:  0.10044866055250168
		 loss:  0.09987696260213852
		 loss:  0.09931141138076782
		 loss:  2.364112377166748
		 loss:  0.10423866659402847
		 loss:  0.10362538695335388
		 loss:  0.10301893949508667
		 loss:  2.329453468322754
		 loss:  0.10807737708091736
		 loss:  0.10742073506116867
		 loss:  2.289975881576538
		 loss:  0.11263064295053482
		 loss:  0.11192060261964798
		 loss:  0.111219123005867
		 loss:  0.11052579432725906
		 loss:  0.10984063893556595
		 loss:  0.1091635599732399
		 loss:  0.10849425941705704
		 loss:  0.10783275216817856
		 loss:  0.10717873275279999
		 loss:  0.10653232038021088
		 loss:  0.10589321702718735
		 loss:  0.1052614226937294
		 loss:  2.3091235160827637
		 loss:  0.11039759963750839
		 loss:  0.10971395671367645
		 loss:  0.10903828591108322
		 loss:  0.10837040096521378
		 loss:  0.10771042108535767
		 loss:  2.2874372005462646

Epoch: 12 -- Train: 0.3404728174209595, Loss: 0.3060201406478882 Accuracy: tensor([0.0455])
running job
		 loss:  0.11293011158704758
		 loss:  0.1122877299785614
		 loss:  0.11165228486061096
		 loss:  0.11102348566055298
		 loss:  2.2583248615264893
		 loss:  0.11580995470285416
		 loss:  0.11513626575469971
		 loss:  0.1144699975848198
		 loss:  0.11381083726882935
		 loss:  0.11315912008285522
		 loss:  0.11251411586999893
		 loss:  2.2457783222198486
		 loss:  0.11734466999769211
		 loss:  0.11665409803390503
		 loss:  0.11597111076116562
		 loss:  0.11529573053121567
		 loss:  0.11462775617837906
		 loss:  0.11396700143814087
		 loss:  0.11331336200237274
		 loss:  0.11266674846410751
		 loss:  0.11202728748321533
		 loss:  0.11139456182718277
		 loss:  0.11076858639717102
		 loss:  0.11014916002750397
		 loss:  2.2657670974731445
		 loss:  0.11490941047668457
		 loss:  0.11424564570188522
		 loss:  0.11358910799026489
		 loss:  0.11293969303369522
		 loss:  0.11229720711708069
		 loss:  0.11166156083345413
		 loss:  0.11103255301713943
		 loss:  0.11041051149368286
		 loss:  0.10979481786489487
		 loss:  0.10918568819761276
		 loss:  0.10858292132616043
		 loss:  0.10798653215169907
		 loss:  0.1073962152004242
		 loss:  2.289616346359253
		 loss:  0.11207354068756104
		 loss:  0.11144031584262848
		 loss:  0.11081383377313614
		 loss:  0.11019378900527954
		 loss:  0.1095806285738945
		 loss:  0.10897371917963028
		 loss:  0.10837318003177643
		 loss:  0.10777892172336578
		 loss:  0.10719062387943268
		 loss:  0.10660862177610397
		 loss:  0.10603249818086624
		 loss:  0.10546226799488068
		 loss:  0.10489782691001892
		 loss:  2.3118255138397217
		 loss:  0.10949825495481491
		 loss:  0.10889226198196411
		 loss:  2.2765755653381348
		 loss:  0.11361496150493622
		 loss:  0.11296524852514267
		 loss:  0.11232246458530426
		 loss:  0.11168651282787323
		 loss:  0.11105751991271973
		 loss:  0.11043506860733032
		 loss:  2.263327121734619
		 loss:  0.11520394682884216
		 loss:  0.11453686654567719
		 loss:  0.11387711763381958
		 loss:  0.1132245883345604
		 loss:  0.11257898807525635
		 loss:  0.1119404286146164
		 loss:  0.11130860447883606
		 loss:  0.11068353801965714
		 loss:  0.11006490886211395
		 loss:  0.10945305973291397
		 loss:  0.10884746164083481
		 loss:  0.10824824124574661
		 loss:  0.10765518993139267
		 loss:  0.10706831514835358
		 loss:  0.10648742318153381
		 loss:  0.10591262578964233
		 loss:  0.10534361004829407
		 loss:  0.10478028655052185
		 loss:  0.10422276705503464
		 loss:  2.3179221153259277
		 loss:  0.10880234092473984
		 loss:  2.277355194091797

Epoch: 13 -- Train: 0.3404426872730255, Loss: 0.3061094284057617 Accuracy: tensor([0.0455])
running job
		 loss:  0.11352227628231049
		 loss:  0.1129383072257042
		 loss:  0.11235985904932022
		 loss:  0.11178704351186752
		 loss:  0.11121976375579834
		 loss:  0.11065781861543655
		 loss:  0.11010132730007172
		 loss:  0.10955007374286652
		 loss:  0.10900397598743439
		 loss:  0.10846313834190369
		 loss:  2.279776096343994
		 loss:  0.1126929521560669
		 loss:  0.11211702227592468
		 loss:  0.11154651641845703
		 loss:  2.2533695697784424
		 loss:  0.11585921794176102
		 loss:  0.11525227874517441
		 loss:  0.11465124785900116
		 loss:  0.11405634880065918
		 loss:  0.11346704512834549
		 loss:  0.11288347095251083
		 loss:  0.11230562627315521
		 loss:  2.2469863891601562
		 loss:  2.206428050994873
		 loss:  0.12172073125839233
		 loss:  0.12105471640825272
		 loss:  0.12039544433355331
		 loss:  0.11974313855171204
		 loss:  0.1190972849726677
		 loss:  0.11845811456441879
		 loss:  0.1178252100944519
		 loss:  0.11719879508018494
		 loss:  2.206913948059082
		 loss:  0.1216585636138916
		 loss:  0.1209932416677475
		 loss:  0.1203346699476242
		 loss:  0.11968285590410233
		 loss:  2.187243700027466
		 loss:  0.12420539557933807
		 loss:  0.12351363897323608
		 loss:  0.12282910197973251
		 loss:  0.1221516951918602
		 loss:  0.12148120999336243
		 loss:  0.12081768363714218
		 loss:  2.1784021854400635
		 loss:  0.12536832690238953
		 loss:  0.1246645450592041
		 loss:  0.12396794557571411
		 loss:  0.1232786625623703
		 loss:  0.12259659916162491
		 loss:  0.12192167341709137
		 loss:  0.12125357240438461
		 loss:  0.12059231847524643
		 loss:  0.11993792653083801
		 loss:  0.11929008364677429
		 loss:  0.11864902079105377
		 loss:  0.11801422387361526
		 loss:  2.2004072666168213
		 loss:  0.12249472737312317
		 loss:  2.1654958724975586
		 loss:  0.12708668410778046
		 loss:  0.12636467814445496
		 loss:  0.12565012276172638
		 loss:  0.12494317442178726
		 loss:  0.12424381822347641
		 loss:  0.12355177849531174
		 loss:  0.12286684662103653
		 loss:  0.12218903750181198
		 loss:  0.12151816487312317
		 loss:  0.12085422873497009
		 loss:  0.12019715458154678
		 loss:  0.11954662203788757
		 loss:  0.11890286952257156
		 loss:  0.11826568841934204
		 loss:  0.11763478070497513
		 loss:  0.11701015383005142
		 loss:  0.11639192700386047
		 loss:  0.11577969044446945
		 loss:  0.11517355591058731
		 loss:  0.11457332968711853
		 loss:  0.11397912353277206
		 loss:  0.11339052766561508
		 loss:  0.1128077581524849
		 loss:  0.11223061382770538
		 loss:  0.11165910959243774

Epoch: 14 -- Train: 0.340017169713974, Loss: 0.30575913190841675 Accuracy: tensor([0.0455])
running job
		 loss:  0.11109303683042526
		 loss:  0.11058823019266129
		 loss:  0.11008787155151367
		 loss:  0.10959174484014511
		 loss:  0.1090998575091362
		 loss:  0.10861211270093918
		 loss:  0.10812841355800629
		 loss:  0.10764876753091812
		 loss:  0.1071733832359314
		 loss:  0.10670173168182373
		 loss:  2.294757127761841
		 loss:  0.11045822501182556
		 loss:  0.10995886474847794
		 loss:  0.10946385562419891
		 loss:  0.10897307842969894
		 loss:  0.1084863469004631
		 loss:  0.10800376534461975
		 loss:  0.10752511769533157
		 loss:  2.2875001430511475
		 loss:  0.11130210012197495
		 loss:  2.2549550533294678
		 loss:  2.2183754444122314
		 loss:  0.11968782544136047
		 loss:  0.11910681426525116
		 loss:  0.11853139102458954
		 loss:  2.195801258087158
		 loss:  0.12256823480129242
		 loss:  2.1644160747528076
		 loss:  0.12669524550437927
		 loss:  0.12604892253875732
		 loss:  0.12540850043296814
		 loss:  0.12477461248636246
		 loss:  0.12414643168449402
		 loss:  0.12352438271045685
		 loss:  0.122907854616642
		 loss:  0.12229737639427185
		 loss:  0.12169254571199417
		 loss:  0.12109316140413284
		 loss:  0.12049953639507294
		 loss:  0.11991127580404282
		 loss:  0.11932837963104248
		 loss:  0.11875075846910477
		 loss:  0.11817830801010132
		 loss:  0.11761114746332169
		 loss:  0.11704928427934647
		 loss:  0.11649209260940552
		 loss:  0.11594010889530182
		 loss:  0.11539313197135925
		 loss:  0.11485085636377335
		 loss:  0.11431338638067245
		 loss:  0.11378072947263718
		 loss:  0.11325269192457199
		 loss:  0.1127292737364769
		 loss:  0.1122104749083519
		 loss:  0.11169599741697311
		 loss:  0.11118616163730621
		 loss:  0.11068065464496613
		 loss:  0.11017948389053345
		 loss:  0.10968254506587982
		 loss:  0.10918985307216644
		 loss:  2.273009777069092
		 loss:  0.1130073070526123
		 loss:  0.11248598992824554
		 loss:  0.11196931451559067
		 loss:  0.11145694553852081
		 loss:  0.11094912141561508
		 loss:  0.11044563353061676
		 loss:  0.1099463701248169
		 loss:  0.10945135354995728
		 loss:  0.10896068066358566
		 loss:  0.10847415775060654
		 loss:  0.10799156129360199
		 loss:  0.10751312971115112
		 loss:  0.10703875124454498
		 loss:  0.10656822472810745
		 loss:  0.10610175877809525
		 loss:  0.10563904792070389
		 loss:  0.10518019646406174
		 loss:  0.10472510755062103
		 loss:  2.3124196529388428
		 loss:  0.10843222588300705
		 loss:  0.10795015096664429
		 loss:  2.283777952194214
		 loss:  0.11173746734857559
		 loss:  0.11122722923755646

Epoch: 15 -- Train: 0.3398417830467224, Loss: 0.30570924282073975 Accuracy: tensor([0.0455])
running job
		 loss:  0.11072131246328354
		 loss:  0.1102697029709816
		 loss:  0.1098216250538826
		 loss:  0.10937730222940445
		 loss:  0.10893609374761581
		 loss:  0.10849811136722565
		 loss:  0.10806379467248917
		 loss:  0.10763249546289444
		 loss:  0.10720454156398773
		 loss:  0.10677973181009293
		 loss:  0.10635805875062943
		 loss:  2.297386646270752
		 loss:  0.1097257062792778
		 loss:  0.10928186774253845
		 loss:  0.10884136706590652
		 loss:  2.275601625442505
		 loss:  0.11226364970207214
		 loss:  2.2464218139648438
		 loss:  0.11576005071401596
		 loss:  0.11526896059513092
		 loss:  2.2215640544891357
		 loss:  0.11882877349853516
		 loss:  0.11831292510032654
		 loss:  0.11780115216970444
		 loss:  0.11729367822408676
		 loss:  2.2052011489868164
		 loss:  0.12089543044567108
		 loss:  0.12036267668008804
		 loss:  0.1198342964053154
		 loss:  0.11931018531322479
		 loss:  0.11879035085439682
		 loss:  0.11827468872070312
		 loss:  0.11776354163885117
		 loss:  0.11725625395774841
		 loss:  0.11675317585468292
		 loss:  0.11625408381223679
		 loss:  0.11575920134782791
		 loss:  0.11526811122894287
		 loss:  0.11478092521429062
		 loss:  0.11429775506258011
		 loss:  2.229520797729492
		 loss:  2.196790933609009
		 loss:  0.12197264283895493
		 loss:  0.12143085151910782
		 loss:  0.12089353054761887
		 loss:  0.12036067247390747
		 loss:  0.11983228474855423
		 loss:  0.11930817365646362
		 loss:  0.11878833919763565
		 loss:  0.11827278882265091
		 loss:  0.11776141822338104
		 loss:  0.1172543466091156
		 loss:  0.11675115674734116
		 loss:  0.11625216901302338
		 loss:  0.1157572865486145
		 loss:  0.11526630073785782
		 loss:  0.11477911472320557
		 loss:  0.1142958402633667
		 loss:  0.11381637305021286
		 loss:  0.113340824842453
		 loss:  0.11286909878253937
		 loss:  0.11240087449550629
		 loss:  2.2452707290649414
		 loss:  0.11589998006820679
		 loss:  0.11540800333023071
		 loss:  0.11491972208023071
		 loss:  0.1144353374838829
		 loss:  0.11395487189292908
		 loss:  0.11347822099924088
		 loss:  0.11300507187843323
		 loss:  0.11253595352172852
		 loss:  0.11207034438848495
		 loss:  0.1116083636879921
		 loss:  0.1111501082777977
		 loss:  0.11069528013467789
		 loss:  0.11024397611618042
		 loss:  0.10979609936475754
		 loss:  0.10935165733098984
		 loss:  0.10891086608171463
		 loss:  0.10847318917512894
		 loss:  0.10803885757923126
		 loss:  0.1076078712940216
		 loss:  0.10718002170324326
		 loss:  0.10675541311502457
		 loss:  2.293865203857422

Epoch: 16 -- Train: 0.33966103196144104, Loss: 0.3056323826313019 Accuracy: tensor([0.0455])
running job
		 loss:  2.2606377601623535
		 loss:  0.11364550143480301
		 loss:  0.11321862787008286
		 loss:  0.1127946600317955
		 loss:  0.1123737096786499
		 loss:  0.11195588111877441
		 loss:  0.11154065281152725
		 loss:  2.2521162033081055
		 loss:  0.11466868221759796
		 loss:  0.11423448473215103
		 loss:  0.11380349844694138
		 loss:  2.233203411102295
		 loss:  0.11697494983673096
		 loss:  0.11652402579784393
		 loss:  0.1160765141248703
		 loss:  0.11563209444284439
		 loss:  0.11519098281860352
		 loss:  0.11475307494401932
		 loss:  0.11431827396154404
		 loss:  0.11388647556304932
		 loss:  0.11345800012350082
		 loss:  0.11303243041038513
		 loss:  0.11260966211557388
		 loss:  0.11219001561403275
		 loss:  0.11177339404821396
		 loss:  0.11135979741811752
		 loss:  0.11094891279935837
		 loss:  0.11054084450006485
		 loss:  0.11013570427894592
		 loss:  0.10973317921161652
		 loss:  0.1093335971236229
		 loss:  0.10893695056438446
		 loss:  0.10854281485080719
		 loss:  2.2778127193450928
		 loss:  2.248012065887451
		 loss:  0.11516527086496353
		 loss:  0.1147274598479271
		 loss:  0.11429286003112793
		 loss:  0.11386136710643768
		 loss:  0.1134328842163086
		 loss:  0.11300751566886902
		 loss:  0.11258506029844284
		 loss:  0.11216551065444946
		 loss:  0.1117490902543068
		 loss:  0.11133559048175812
		 loss:  2.253852128982544
		 loss:  2.224217176437378
		 loss:  0.1180875301361084
		 loss:  0.117628313601017
		 loss:  0.11717291921377182
		 loss:  0.11672071367502213
		 loss:  0.11627159267663956
		 loss:  0.11582587659358978
		 loss:  0.11538346856832504
		 loss:  0.11494405567646027
		 loss:  0.11450784653425217
		 loss:  0.1140749603509903
		 loss:  0.11364497244358063
		 loss:  2.2345147132873535
		 loss:  0.11681320518255234
		 loss:  0.11636359989643097
		 loss:  0.11591718345880508
		 loss:  0.11547396332025528
		 loss:  0.11503416299819946
		 loss:  0.11459735780954361
		 loss:  0.11416365951299667
		 loss:  0.11373306810855865
		 loss:  0.11330538243055344
		 loss:  0.11288081109523773
		 loss:  0.1124594658613205
		 loss:  0.11204072088003159
		 loss:  0.11162520945072174
		 loss:  0.11121250689029694
		 loss:  2.2548937797546387
		 loss:  0.1143341213464737
		 loss:  0.11390221863985062
		 loss:  0.11347364634275436
		 loss:  0.11304797232151031
		 loss:  0.1126251071691513
		 loss:  0.11220536381006241
		 loss:  0.11178874969482422
		 loss:  0.11137483268976212
		 loss:  0.11096373945474625
		 loss:  0.11055589467287064
		 loss:  0.110150545835495

Epoch: 17 -- Train: 0.339530348777771, Loss: 0.30558374524116516 Accuracy: tensor([0.0455])
running job
		 loss:  0.10974813997745514
		 loss:  0.1093880906701088
		 loss:  0.10903080552816391
		 loss:  0.10867521166801453
		 loss:  0.10832205414772034
		 loss:  0.10797123610973358
		 loss:  0.10762232542037964
		 loss:  0.10727554559707642
		 loss:  0.10693099349737167
		 loss:  2.291600227355957
		 loss:  0.10966096818447113
		 loss:  0.10930174589157104
		 loss:  2.2708919048309326
		 loss:  0.11207397282123566
		 loss:  2.2472710609436035
		 loss:  0.11489453166723251
		 loss:  0.1145024225115776
		 loss:  0.11411260813474655
		 loss:  0.11372530460357666
		 loss:  0.11334050446748734
		 loss:  0.11295822262763977
		 loss:  0.11257824301719666
		 loss:  0.11220067739486694
		 loss:  0.11182563006877899
		 loss:  0.11145278811454773
		 loss:  0.11108237504959106
		 loss:  0.11071427166461945
		 loss:  0.11034849286079407
		 loss:  0.10998482257127762
		 loss:  2.265012502670288
		 loss:  0.11276920884847641
		 loss:  0.11239054054021835
		 loss:  0.11201418191194534
		 loss:  0.11164023727178574
		 loss:  0.11126860976219177
		 loss:  2.254070520401001
		 loss:  2.2273919582366943
		 loss:  0.11732654273509979
		 loss:  0.1169184222817421
		 loss:  0.11651288717985153
		 loss:  2.2107093334198
		 loss:  0.1194090023636818
		 loss:  0.11898732930421829
		 loss:  0.11856823414564133
		 loss:  0.11815193295478821
		 loss:  0.11773842573165894
		 loss:  0.11732760816812515
		 loss:  0.11691927164793015
		 loss:  0.11651405692100525
		 loss:  0.11611122637987137
		 loss:  0.11571088433265686
		 loss:  0.11531336605548859
		 loss:  0.11491844058036804
		 loss:  0.1145259216427803
		 loss:  0.11413589864969254
		 loss:  0.11374849826097488
		 loss:  0.11336349695920944
		 loss:  0.11298111081123352
		 loss:  0.11260103434324265
		 loss:  0.11222337186336517
		 loss:  0.1118481233716011
		 loss:  2.249173641204834
		 loss:  0.11466474831104279
		 loss:  0.11427393555641174
		 loss:  0.11388573050498962
		 loss:  0.113499715924263
		 loss:  0.1131163239479065
		 loss:  0.11273545026779175
		 loss:  0.11235698312520981
		 loss:  0.11198082566261292
		 loss:  0.11160718649625778
		 loss:  0.11123565584421158
		 loss:  0.11086665093898773
		 loss:  0.11049985885620117
		 loss:  0.11013549566268921
		 loss:  0.10977334529161453
		 loss:  0.10941341519355774
		 loss:  0.10905560851097107
		 loss:  0.10870012640953064
		 loss:  0.10834665596485138
		 loss:  0.10799574106931686
		 loss:  0.10764673352241516
		 loss:  0.10729984939098358
		 loss:  0.10695509612560272
		 loss:  2.291388750076294

Epoch: 18 -- Train: 0.3393554985523224, Loss: 0.3055758476257324 Accuracy: tensor([0.0455])
running job
		 loss:  0.10968542844057083
		 loss:  2.2672741413116455
		 loss:  0.11218351870775223
		 loss:  0.11184588074684143
		 loss:  0.111510269343853
		 loss:  0.11117645353078842
		 loss:  0.11084455996751785
		 loss:  0.11051469296216965
		 loss:  0.11018653213977814
		 loss:  0.10986029356718063
		 loss:  2.2657718658447266
		 loss:  0.11236102879047394
		 loss:  0.11202238500118256
		 loss:  0.11168576776981354
		 loss:  0.11135094612836838
		 loss:  0.11101825535297394
		 loss:  0.1106872707605362
		 loss:  0.1103580966591835
		 loss:  0.11003084480762482
		 loss:  2.2643094062805176
		 loss:  0.11253424733877182
		 loss:  0.11219470947980881
		 loss:  0.11185707896947861
		 loss:  0.11152125149965286
		 loss:  0.11118744313716888
		 loss:  0.11085566133260727
		 loss:  0.11052557826042175
		 loss:  0.11019742488861084
		 loss:  0.10987097024917603
		 loss:  0.10954633355140686
		 loss:  0.10922373086214066
		 loss:  0.10890273749828339
		 loss:  0.10858366638422012
		 loss:  0.10826621204614639
		 loss:  0.10795068740844727
		 loss:  0.10763656347990036
		 loss:  0.10732448101043701
		 loss:  0.1070140078663826
		 loss:  0.10670537501573563
		 loss:  2.2932937145233154
		 loss:  0.10915554314851761
		 loss:  0.10883516818284988
		 loss:  2.2746222019195557
		 loss:  0.11131959408521652
		 loss:  0.11098678410053253
		 loss:  0.1106560006737709
		 loss:  0.11032724380493164
		 loss:  0.11000008881092072
		 loss:  0.10967475175857544
		 loss:  0.10935133695602417
		 loss:  0.10902974009513855
		 loss:  0.10870985686779022
		 loss:  0.10839179903268814
		 loss:  0.108075350522995
		 loss:  0.10776083171367645
		 loss:  0.10744782537221909
		 loss:  0.10713675618171692
		 loss:  0.10682740807533264
		 loss:  0.1065196767449379
		 loss:  0.10621356964111328
		 loss:  0.1059090867638588
		 loss:  0.1056063324213028
		 loss:  0.10530530661344528
		 loss:  0.1050059124827385
		 loss:  0.10470803827047348
		 loss:  2.3111648559570312
		 loss:  0.10712572187185287
		 loss:  0.10681626200675964
		 loss:  2.2923097610473633
		 loss:  0.10926862061023712
		 loss:  0.10894731432199478
		 loss:  0.10862794518470764
		 loss:  0.1083102822303772
		 loss:  0.10799434781074524
		 loss:  2.2819461822509766
		 loss:  0.11046559363603592
		 loss:  0.11013752222061157
		 loss:  0.10981147736310959
		 loss:  0.10948725044727325
		 loss:  0.10916505753993988
		 loss:  2.2717654705047607
		 loss:  0.1116548478603363
		 loss:  0.1113201230764389
		 loss:  0.11098753660917282
		 loss:  0.11065664142370224

Epoch: 19 -- Train: 0.33924007415771484, Loss: 0.30565765500068665 Accuracy: tensor([0.0455])
running job
		 loss:  2.2589573860168457
		 loss:  0.11288272589445114
		 loss:  0.11257525533437729
		 loss:  0.11226940155029297
		 loss:  0.11196515709161758
		 loss:  0.11166241765022278
		 loss:  2.250140428543091
		 loss:  0.11393604427576065
		 loss:  0.11362305283546448
		 loss:  0.113311767578125
		 loss:  0.11300230026245117
		 loss:  0.11269401758909225
		 loss:  2.241469144821167
		 loss:  0.11498230695724487
		 loss:  0.11466389894485474
		 loss:  0.11434730142354965
		 loss:  0.11403220146894455
		 loss:  0.1137189194560051
		 loss:  0.11340712755918503
		 loss:  0.11309695243835449
		 loss:  0.1127883791923523
		 loss:  0.11248141527175903
		 loss:  0.11217606067657471
		 loss:  2.2458128929138184
		 loss:  0.11445692181587219
		 loss:  0.1141413226723671
		 loss:  0.11382743716239929
		 loss:  0.11351504176855087
		 loss:  0.11320425570011139
		 loss:  0.11289528757333755
		 loss:  0.11258772015571594
		 loss:  0.11228176206350327
		 loss:  0.11197751760482788
		 loss:  0.11167467385530472
		 loss:  0.11137344688177109
		 loss:  0.11107362806797028
		 loss:  0.1107754185795784
		 loss:  2.257662534713745
		 loss:  0.11303658038377762
		 loss:  0.11272841691970825
		 loss:  0.11242175847291946
		 loss:  0.11211659759283066
		 loss:  0.11181315779685974
		 loss:  0.11151111871004105
		 loss:  0.11121080070734024
		 loss:  0.11091189086437225
		 loss:  0.11061448603868484
		 loss:  0.11031860113143921
		 loss:  0.11002422869205475
		 loss:  2.2640843391418457
		 loss:  0.11227472871541977
		 loss:  0.11197048425674438
		 loss:  0.11166753619909286
		 loss:  0.11136651784181595
		 loss:  0.11106658726930618
		 loss:  0.11076848208904266
		 loss:  0.11047200113534927
		 loss:  0.11017681658267975
		 loss:  0.10988293588161469
		 loss:  2.265296697616577
		 loss:  0.1121315136551857
		 loss:  0.11182797700166702
		 loss:  0.11152593791484833
		 loss:  0.11122552305459976
		 loss:  0.11092650890350342
		 loss:  0.1106291115283966
		 loss:  0.11033311486244202
		 loss:  2.2614383697509766
		 loss:  0.11258804053068161
		 loss:  0.11228197067975998
		 loss:  0.11197773367166519
		 loss:  0.11167488992214203
		 loss:  0.1113736629486084
		 loss:  0.11107383668422699
		 loss:  0.11077573895454407
		 loss:  0.11047904193401337
		 loss:  0.11018386483192444
		 loss:  0.10988998413085938
		 loss:  0.10959793627262115
		 loss:  0.109307199716568
		 loss:  0.10901787132024765
		 loss:  2.2727599143981934
		 loss:  0.11125410348176956
		 loss:  0.1109548881649971
		 loss:  0.11065728217363358

Epoch: 20 -- Train: 0.3390948474407196, Loss: 0.3056619465351105 Accuracy: tensor([0.0455])
